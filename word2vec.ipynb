{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Word2vec</center>\n",
    "### <center>Calvin Chi</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "This notebook is largely based on the paper [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) [1]. A few high level notes and tips from the paper include:\n",
    "\n",
    "+ The Skip-gram architecture in Figure 1 of Mikolov _et_ _at_. actually only involves training of two matrices - the input and output matrices. The Skip-gram architecture does not involve a separate output matrix for each output context word. Rather, Skip-Gram model dictates how an input word is paired output words that are in its context.\n",
    "+ Since the Skip-gram model fits a one-to-many mapping of words, the point is not to get any particular mapping between a pair of words correct, but rather to use this fitting procedure to fit a good input word matrix whose columns (or rows) are vector representations of words that capture syntactic or semantic word relationships.\n",
    "+ The Skip-gram model assumes distributional hypothesis, which assumes that a word's syntactic or semantic meaning is correlated with the type of words it is surrounded by.\n",
    "+ Larger context window $\\pm m$ results in more training examples and thus can lead to a higher accuracy, at the expense of the training time.\n",
    "+ The Skip-gram model presented by Mikolov _et_ _at_. is called word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import math\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train word2vec using [IMDB movie review dataset](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/data). Standard cleaning steps for the text dataset is adopted from this [tutorial](https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/).\n",
    "\n",
    "+ Remove punctuations:\n",
    "```\n",
    "!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\n",
    "```\n",
    "+ Remove characters not alphabetic.\n",
    "+ Remove stop words from [nltk](https://www.nltk.org/). Stop words usually are most common words in a language filtered out before natural language processing. \n",
    "+ Remove tokens, or fundamental units of text, with length one.\n",
    "\n",
    "Mikolov _et_ _at_. removed some frequent words from the text, since frequent words provide less information value than rare words. To implement this, each words $w_{i}$ in training is discarded with probability \n",
    "\n",
    "$$P(w_{i}) = 1 - \\sqrt{\\frac{t}{f(w_{i})}},$$\n",
    "\n",
    "where $f(w_{i})$ is frequency of word $w_{i}$, and $t$ is a chosen threshold, set to $10^{-5}$ in the paper. Another data preprocessing task from Mikolov _et_ _at_. is removing all words that occured less than 5 times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename, directory = \"data/\"):\n",
    "    # open the file as read only\n",
    "    data = pd.read_csv(directory + filename, sep = \",\")\n",
    "    reviews = list(data['review'])\n",
    "    return reviews\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(reviews, stopwords):\n",
    "    t = 10**(-5)\n",
    "    \n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    cleaned_reviews = []\n",
    "    for review in reviews:\n",
    "        # split into tokens by white space\n",
    "        review_tokens = review.split()\n",
    "        review_tokens = [w.translate(table) for w in review_tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        review_tokens = [word.lower() for word in review_tokens if word.isalpha()]\n",
    "        review_tokens = [w for w in review_tokens if not w in stop_words]\n",
    "        # filter out short tokens\n",
    "        review_tokens = [word for word in review_tokens if len(word) > 1]\n",
    "        cleaned_reviews.append(review_tokens)\n",
    "    tokens = [token for review_tokens in cleaned_reviews for token in review_tokens]\n",
    "        \n",
    "    # subsampling of frequent words\n",
    "    word_counts = Counter(tokens)\n",
    "    total_count = len(word_counts)\n",
    "    drop_prob = {word: 1 - math.sqrt(t / (count / total_count)) for word, \n",
    "                 count in word_counts.items()}\n",
    "    reviews_subsampling = []\n",
    "    for i in range(len(cleaned_reviews)):\n",
    "        review = cleaned_reviews[i]\n",
    "        review_tokens = [word for word in review if np.random.random() <= drop_prob[word] and word_counts[word] >= 5]\n",
    "        reviews_subsampling.append(review_tokens)\n",
    "    return reviews_subsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = load_doc(\"IMDB_Dataset.csv\")\n",
    "cleaned_reviews = clean_doc(reviews, stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integer encode tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [token for review in cleaned_reviews for token in review]\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_tokens = label_encoder.fit_transform(tokens)\n",
    "article_lengths = [len(review) for review in cleaned_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split tokens by reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_reviews = []\n",
    "start = 0\n",
    "for i in range(len(article_lengths)):\n",
    "    encoded_reviews.append(encoded_tokens[start:(start + article_lengths[i])])\n",
    "    start = start + article_lengths[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training(encoded_reviews, window):\n",
    "    X, Y = [], []\n",
    "    for review in encoded_reviews:\n",
    "        for i in range(len(review)):\n",
    "            for j in range(max(0, i - window), min(len(review), i + window), 1):\n",
    "                if j != i:\n",
    "                    X.append(review[i])\n",
    "                    Y.append(review[j])\n",
    "    X, Y = np.array(X), np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis = 1)\n",
    "    return (X, Y)\n",
    "\n",
    "X, Y = generate_training(encoded_reviews, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notations and explainations are based off of this [note](https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf). Let $V$ be the vocabulary size, $h$ be word vector dimension, $V \\in \\mathbb{R}^{V \\times h}$ be the input word matrix, and $U \\in \\mathbb{R}^{V \\times h}$ be the output word matrix. \n",
    "\n",
    "+ Furthermore, let $u_{i}$ be the $i$-th row of $U$ and $\\tilde{v} = V^{\\top}x \\in \\mathbb{R}^{h}$, where $x \\in \\mathbb{R}^{V}$ is the one-hot encoding of word $w$.\n",
    "+ From word vector $\\tilde{v}$, generate score vector $z = U\\tilde{v}$, from which a probability distribution over words can be genreated with $\\hat{y}$ = softmax$(z)$.\n",
    "\n",
    "Given $T$ words, the log-likelihood to maximize for the Skip-gram model is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{\\ell}(V, U) &= \\log\\prod_{t = 1}^{T}P(w_{t - m}, \\dots ,w_{t - 1}, w_{t + 1}, \\dots ,w_{t + m} | w_{t})\\\\\n",
    "&= \\log \\prod_{t = 1}^{T} \\prod_{j = 0; j \\neq t}^{2m}P(w_{t - m + j}|w_{t})\\\\\n",
    "&= \\log \\prod_{t = 1}^{T}\\prod_{j = 0; j \\neq t}^{2m}\\frac{\\exp(u_{t - m + j}^{\\top}\\tilde{v}_{t})}{\\sum_{k = 1}^{V}\\exp(u_{k}^{\\top}\\tilde{v}_{t})}\\\\\n",
    "&= \\sum_{t = 1}^{T}\\left(\\sum_{j = 0; j \\neq t}^{2m}u_{t - m + j}^{\\top}v_{t} - 2m\\log \\sum_{k = 1}^{V}\\exp(u_{k}^{\\top}v_{t})\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "assuming strong independence assumption between target words conditional on context word $w_{t}$. Maximizing log-likelihood can be achieved by minimizing negative log-likelihood or the average negative log-likelihood over words.\n",
    "\n",
    "$$\n",
    "\\mathcal{J}(V, U) = -\\frac{1}{T}\\sum_{t = 1}^{T}\\left(\\sum_{j = 0; j \\neq t}^{2m}u_{t - m + j}^{\\top}v_{t} + 2m\\log \\sum_{k = 1}^{V}\\exp(u_{k}^{\\top}v_{t})\\right)\n",
    "$$\n",
    "\n",
    "The computational problem with the above formulation is that the vocabulary size $V$ is typically huge. An alternative is to optimize a cost function that approximates $\\mathcal{J}(V, U)$, yet is computationally more efficient via negative sampling. In negative sampling, target words are distinguished from non-target words using logistic regression. In other words, a pairing between context and target word has positive class label and a pairing between context and non-target word has negative class label. Let \n",
    "\n",
    "$$P(Y = 1 | w, c) = \\sigma(u_{w}^{\\top}\\tilde{v}_{c}) = \\frac{1}{1 + \\exp(-u_{w}^{\\top}\\tilde{v}_{c})},$$\n",
    "\n",
    "where $Y$ is class label, $w$ is target word, and $c$ is context word. Let $D$ denote positive class dataset and $\\tilde{D}$ denote negative class dataset. Then the cost function under the formulation is\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{J}(V, U) &= -\\log\\left(\\prod_{(w, c) \\in D}P(D = 1 | w, c)\\prod_{(w, c) \\in \\tilde{D}}P(D = 0|w, c)\\right)\\\\\n",
    "&= -\\sum_{(w, c) \\in D}\\log\\left(\\frac{1}{1 + \\exp(-u_{w}^{\\top}v_{c})}\\right) - \\sum_{(w, c) \\in \\tilde{D}}\\log\\left(\\frac{1}{1 + \\exp(u_{w}^{\\top}v_{c})}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In practice, it may not be necessary to construct entire $\\tilde{D}$ for decent performance. Thus, the practical implementation minimizes the following cost function\n",
    "\n",
    "$$\n",
    "\\mathcal{J}(V, U) = -\\frac{1}{T}\\sum_{t = 1}^{T}\\left(\\sum_{j = 0; j \\neq t}^{2m}\\sigma(u_{t - m + j}^{\\top}\\tilde{v}_{t}) - \\sum_{k = 1}^{K}\\log \\sigma(-\\tilde{u}_{k}^{\\top}\\tilde{v}_{t})\\right)\n",
    "$$\n",
    "\n",
    "where $\\tilde{u}_{k}$ corresponds to non-target words of $w_{t}$. Negative sampling refers to the sampling of $K$ word pairs of negative class, where $K = 10$ is chosen. \n",
    "\n",
    "In Tensorflow 2.1.0., the negative sampling defined above is assumed to be implemented by $\\texttt{tf.compat.v1.nn.sampled_softmax_loss()}$. Note that softmax with 2 classes is equivalent to logistic regression\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(Y = 1, z_{1}, z_{2}) &= \\frac{e^{z_{1}}}{e^{z_1} + e^{z_2}}\\\\\n",
    "&= \\frac{1}{e^{z_2 - z_1} + 1}\\\\\n",
    "&= \\frac{1}{e^{-z} + 1}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $z = z_1 - z_2$. A few implementation notes:\n",
    "\n",
    "+ Input word matrix will be initialized with values drawn from uniform distribution in the range [-1, 1).\n",
    "+ Output word matrix will be initialized with values drawn from a normal distribution mean -1 and standard deviation 1. Values whose magnitude is more than 2 standard deviations from the mean are dropped and re-picked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class word2vec(object):\n",
    "    \n",
    "    def __init__(self, sess, vocab_size, n_embedding, batch_size = 512):\n",
    "        self.sess = sess\n",
    "        self.V = vocab_size\n",
    "        self.h = n_embedding\n",
    "        self.batch_size = batch_size\n",
    "        self.__build_computational_graph__()\n",
    "        self.__define_train_ops__()\n",
    "        self.sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        self.checkpoint_dir = \"checkpoints/\"\n",
    "        \n",
    "    def __build_computational_graph__(self):\n",
    "        self.input_ph = tf.compat.v1.placeholder(dtype=tf.int64, shape=[None])\n",
    "        self.output_ph = tf.compat.v1.placeholder(dtype=tf.int64, shape=[None, 1])\n",
    "        \n",
    "        embedding = tf.Variable(tf.random.uniform((self.V, self.h), -1, 1))\n",
    "        self.embed = tf.nn.embedding_lookup(embedding, self.input_ph)\n",
    "\n",
    "        self.softmax_w = tf.Variable(tf.random.truncated_normal((self.V, self.h), -1, 1))\n",
    "        self.softmax_b = tf.Variable(tf.zeros(self.V), name=\"softmax_bias\")\n",
    "        \n",
    "        self.train_loss = tf.reduce_mean(tf.compat.v1.nn.sampled_softmax_loss(weights=self.softmax_w,\n",
    "                                                                              biases=self.softmax_b,\n",
    "                                                                              labels=self.output_ph,\n",
    "                                                                              inputs=self.embed,\n",
    "                                                                              num_sampled = 10,\n",
    "                                                                              num_classes = self.V))\n",
    "        \n",
    "    def __define_train_ops__(self):\n",
    "        self.opt = tf.compat.v1.train.AdamOptimizer().minimize(self.train_loss)\n",
    "        logits = tf.matmul(self.embed, tf.transpose(self.softmax_w))\n",
    "        logits = tf.nn.bias_add(logits, self.softmax_b)\n",
    "        labels_one_hot = tf.one_hot(self.output_ph, self.V)\n",
    "        self.eval_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, \n",
    "                                                                                logits=logits))\n",
    "        \n",
    "    def train(self, X, Y, verbose = True, epochs = 10):\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(\"epoch {0} / {1}\".format(epoch, epochs))\n",
    "            for i in range(math.ceil(len(X) / self.batch_size)):\n",
    "                Xbatch = X[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                Ybatch = Y[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "                _, loss_batch = self.sess.run([self.opt, self.train_loss], \n",
    "                                              feed_dict={self.input_ph: Xbatch, self.output_ph: Ybatch})\n",
    "                if i % 1000 == 0:\n",
    "                    softmax_loss = self.sess.run(self.eval_loss, feed_dict={self.input_ph: Xbatch, \n",
    "                                                                            self.output_ph: Ybatch})\n",
    "                    print('sampled softmax loss: {:.3f} | softmax loss: {:.3f}'.format(loss_batch, \n",
    "                                                                                       softmax_loss))\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                saver.save(self.sess, save_path='models/word2vec.ckpt')\n",
    "            \n",
    "            shuffle_idx = np.random.permutation(len(X))\n",
    "            X, Y = X[shuffle_idx], Y[shuffle_idx]\n",
    "    \n",
    "    def get_embedding(self, X):\n",
    "        return self.sess.run(self.embed, feed_dict = {self.input_ph: X})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize graph, session, and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "sess = tf.compat.v1.Session()\n",
    "V = len(np.unique(X))\n",
    "\n",
    "model = word2vec(sess, V, n_embedding=300, batch_size=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training will print the following progress messages. In addition to printing the training loss based on softmax, we also print the original loss, which the training loss approximates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(X, Y, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "epoch 0 / 10\n",
    "sampled softmax loss: 11.395 | softmax loss: 37.042\n",
    "sampled softmax loss: 10.140 | softmax loss: 35.712\n",
    "sampled softmax loss: 10.435 | softmax loss: 34.629\n",
    "sampled softmax loss: 10.022 | softmax loss: 32.538\n",
    "sampled softmax loss: 7.056 | softmax loss: 28.374\n",
    "sampled softmax loss: 3.617 | softmax loss: 26.147\n",
    "sampled softmax loss: 5.020 | softmax loss: 26.835\n",
    "sampled softmax loss: 3.111 | softmax loss: 28.299\n",
    "sampled softmax loss: 3.713 | softmax loss: 29.446\n",
    "sampled softmax loss: 4.481 | softmax loss: 29.410\n",
    "sampled softmax loss: 3.502 | softmax loss: 29.332\n",
    "sampled softmax loss: 0.908 | softmax loss: 21.718\n",
    "sampled softmax loss: 3.204 | softmax loss: 28.391\n",
    "sampled softmax loss: 1.376 | softmax loss: 22.891\n",
    "sampled softmax loss: 3.436 | softmax loss: 23.964\n",
    "sampled softmax loss: 1.523 | softmax loss: 21.603\n",
    "sampled softmax loss: 2.464 | softmax loss: 21.597\n",
    "sampled softmax loss: 1.090 | softmax loss: 17.275\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Word Vector Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of the word vectors are assessed with analogical reasoning tasks introduced by Mikolov _et_ _at_.[2]. The reasoning task is successful by satisfying an equation such as the following\n",
    "\n",
    "```\n",
    "vec(queen) - vec(woman) = vec(king) - vec(man)\n",
    "```\n",
    "\n",
    "where `vec(queen)` means the word vector of \"queen\". We evaluate performance of this task by testing whether\n",
    "\n",
    "```\n",
    "vec(queen) - vec(woman) + vec(man) = ?\n",
    "```\n",
    "\n",
    "yields `vec(king)` as the closest vector out of all word vectors in terms of cosine similarity. Obtain word vectors for all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/word2vec.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/word2vec.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "sess = tf.compat.v1.Session()\n",
    "\n",
    "model = word2vec(sess, V, n_embedding=300, batch_size=2048)\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "saver.restore(sess, 'models/word2vec.ckpt')\n",
    "\n",
    "unique_tokens = np.unique(X)\n",
    "word_vectors = model.get_embedding(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for analogical reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    return x.dot(y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "def closest_k_vectors(word_vectors, words, label_encoder, model, k = 10):\n",
    "    encoded_words = label_encoder.transform(words)\n",
    "    embeddings = model.get_embedding(encoded_words)\n",
    "    vector = embeddings[0] - embeddings[1] + embeddings[2]\n",
    "    \n",
    "    heap = []\n",
    "    # get closest k vectors \n",
    "    for i in range(len(word_vectors)):\n",
    "        if i not in encoded_words:\n",
    "            heapq.heappush(heap, (cosine_similarity(word_vectors[i], vector), i))\n",
    "            if len(heap) > k:\n",
    "                heapq.heappop(heap)\n",
    "    \n",
    "    heap.sort(reverse = True)\n",
    "    return list(label_encoder.inverse_transform([w for s, w in heap]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we illustrate that our word vectors can satisfy the following relationships\n",
    "\n",
    "1. queen - woman = king - man\n",
    "2. brother - boy = sister - girl\n",
    "3. woman - man = uncle - aunt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen - woman + man = ?\n",
      "Top 10 closest words in descending order:  ['last', 'king', 'powerful', 'almost', 'title', 'wars', 'one', 'war', 'back', 'better']\n",
      " \n",
      "brother - boy + girl = ?\n",
      "Top 10 closest words in descending order:  ['sister', 'shes', 'really', 'also', 'married', 'woman', 'playing', 'things', 'get', 'named']\n",
      " \n",
      "woman - man + uncle = ?\n",
      "Top 10 closest words in descending order:  ['wife', 'played', 'family', 'old', 'young', 'aunt', 'falls', 'mother', 'lady', 'married']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "wordgroups = [[\"queen\", \"woman\", \"man\"], [\"brother\", \"boy\", \"girl\"],\n",
    "              [\"woman\", \"man\", \"uncle\"]]\n",
    "\n",
    "for group in wordgroups:\n",
    "    print(\"{0} - {1} + {2} = ?\".format(group[0], group[1], group[2]))\n",
    "    print(\"Top 10 closest words in descending order: \", \n",
    "          closest_k_vectors(word_vectors, group, label_encoder, model, k = 10))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, our word vectors fail at satisfying other relationships presented by Mikolov _et_ _at_.[2]. Underperformance could be due to size of training dataset, type of training dataset, and training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. Mikolov, Tomas, et al. \"Distributed representations of words and phrases and their compositionality.\" Advances in neural information processing systems. 2013.\n",
    "2. Mikolov, Tomas, et al. \"Efficient estimation of word representations in vector space.\" arXiv preprint arXiv:1301.3781 (2013)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
